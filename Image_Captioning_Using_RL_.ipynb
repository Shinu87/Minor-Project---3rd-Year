{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wIVyMv51Xzu8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras import optimizers\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import time\n",
        "import glob\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D8mUVX16X1z4"
      },
      "outputs": [],
      "source": [
        "def download_and_extract_dataset():\n",
        "    print(\"Downloading Flickr8k dataset...\")\n",
        "\n",
        "    # Download Flickr8k images\n",
        "    image_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
        "    text_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    # Download and extract image dataset\n",
        "    if not os.path.exists(\"data/Flickr8k_Dataset\"):\n",
        "        print(\"Downloading image dataset...\")\n",
        "        r = requests.get(image_url)\n",
        "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "        z.extractall(\"data\")\n",
        "        print(\"Image dataset downloaded and extracted.\")\n",
        "    else:\n",
        "        print(\"Image dataset already exists.\")\n",
        "\n",
        "    # Download and extract text dataset\n",
        "    if not os.path.exists(\"data/Flickr8k_text\"):\n",
        "        print(\"Downloading text dataset...\")\n",
        "        r = requests.get(text_url)\n",
        "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "        z.extractall(\"data\")\n",
        "        print(\"Text dataset downloaded and extracted.\")\n",
        "    else:\n",
        "        print(\"Text dataset already exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yvroUxutX4dw"
      },
      "outputs": [],
      "source": [
        "def load_descriptions(filename):\n",
        "    file = open(filename, 'r')\n",
        "    doc = file.read()\n",
        "    file.close()\n",
        "\n",
        "    descriptions = {}\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        image_id = image_id.split('.')[0]\n",
        "        image_desc = ' '.join(image_desc)\n",
        "\n",
        "        if image_id not in descriptions:\n",
        "            descriptions[image_id] = []\n",
        "        descriptions[image_id].append(image_desc)\n",
        "\n",
        "    return descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rnwFPmd_X6Ow"
      },
      "outputs": [],
      "source": [
        "def clean_descriptions(descriptions):\n",
        "    import re\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    # Download stopwords\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            # Convert to lowercase\n",
        "            desc = desc.lower()\n",
        "            # Remove punctuation\n",
        "            desc = re.sub('[^a-zA-Z]', ' ', desc)\n",
        "            # Remove single characters\n",
        "            desc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', desc)\n",
        "            # Remove multiple spaces\n",
        "            desc = re.sub(r'\\s+', ' ', desc)\n",
        "            # Remove stopwords (commented out to keep more natural captions)\n",
        "            # desc = ' '.join([word for word in desc.split() if word not in stop_words])\n",
        "            # Store cleaned description\n",
        "            desc_list[i] = desc\n",
        "\n",
        "    return descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "51CThxdHX8AE"
      },
      "outputs": [],
      "source": [
        "def save_descriptions(descriptions, filename):\n",
        "    lines = []\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aZxVv_aYX90-"
      },
      "outputs": [],
      "source": [
        "def load_set(filename):\n",
        "    file = open(filename, 'r')\n",
        "    doc = file.read()\n",
        "    file.close()\n",
        "\n",
        "    dataset = []\n",
        "    for line in doc.split('\\n'):\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        image_id = line.split('.')[0]\n",
        "        dataset.append(image_id)\n",
        "\n",
        "    return set(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "msYGhScfX_SA"
      },
      "outputs": [],
      "source": [
        "def load_clean_descriptions(filename, dataset):\n",
        "    file = open(filename, 'r')\n",
        "    doc = file.read()\n",
        "    file.close()\n",
        "\n",
        "    descriptions = {}\n",
        "    for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        if image_id in dataset:\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = []\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "\n",
        "    return descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r6czPN63cvQM"
      },
      "outputs": [],
      "source": [
        "def extract_features(directory, sample_size=None):\n",
        "    model = VGG16()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    print(\"VGG16 model loaded for feature extraction\")\n",
        "\n",
        "    features = {}\n",
        "    image_paths = glob.glob(os.path.join(directory, '*.jpg'))\n",
        "\n",
        "    if sample_size is not None and sample_size < len(image_paths):\n",
        "        print(f\"Using a sample of {sample_size} images from {len(image_paths)} total images\")\n",
        "        image_paths = random.sample(image_paths, sample_size)\n",
        "\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing image {i}/{len(image_paths)}\")\n",
        "\n",
        "        image_id = os.path.basename(image_path).split('.')[0]\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess the image\n",
        "            img = load_img(image_path, target_size=(224, 224))\n",
        "            img = img_to_array(img)\n",
        "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
        "            img = tf.keras.applications.vgg16.preprocess_input(img)\n",
        "\n",
        "            # Extract features\n",
        "            feature = model.predict(img, verbose=0)\n",
        "\n",
        "            # Store feature vector\n",
        "            features[image_id] = feature.flatten()  # Flatten to ensure consistent shape\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Features extracted for {len(features)} images\")\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5Sz2OS-9cySl"
      },
      "outputs": [],
      "source": [
        "def create_sequences(tokenizer, max_length, descriptions, features, vocab_size):\n",
        "    X1, X2, y = [], [], []\n",
        "\n",
        "    for image_id, desc_list in descriptions.items():\n",
        "        if image_id not in features:\n",
        "            continue\n",
        "\n",
        "        feature = features[image_id]\n",
        "\n",
        "        for desc in desc_list:\n",
        "            # Tokenize\n",
        "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\n",
        "            # Create input-output pairs\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FQ-MePR4c1Mh"
      },
      "outputs": [],
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    # Feature extractor model\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "    # Sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    # Disable cuDNN by setting use_cudnn=False\n",
        "    se3 = LSTM(256, use_cudnn=False)(se2)  # Add this parameter\n",
        "\n",
        "    # Rest of model definition remains the same\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "11rmn1acc4Dm"
      },
      "outputs": [],
      "source": [
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    \"\"\"\n",
        "    Generate a description for an image using the trained model\n",
        "    \"\"\"\n",
        "    # Start the generation process with the start token\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    # Iterate until we reach the end token or max length\n",
        "    for i in range(max_length):\n",
        "        # Encode the current input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # Pad the sequence\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\n",
        "        # Reshape photo features if needed\n",
        "        if isinstance(photo, np.ndarray):\n",
        "            if len(photo.shape) == 1:  # If 1D array (4096,)\n",
        "                photo_input = photo.reshape(1, -1)  # Reshape to (1, 4096)\n",
        "            elif len(photo.shape) == 2 and photo.shape[0] > 1:  # If (4096, 1)\n",
        "                photo_input = np.transpose(photo)  # Transpose to (1, 4096)\n",
        "            else:\n",
        "                photo_input = photo  # Already shaped as (1, 4096)\n",
        "        else:\n",
        "            # Handle case where photo might be a tensor\n",
        "            photo_input = photo\n",
        "\n",
        "        # Predict the next word\n",
        "        yhat = model.predict([photo_input, sequence], verbose=0)\n",
        "\n",
        "        # Convert prediction to word index\n",
        "        yhat = np.argmax(yhat)\n",
        "\n",
        "        # Map the index to a word\n",
        "        word = tokenizer.index_word.get(yhat, '')\n",
        "\n",
        "        # Stop if word can't be mapped or we reach the end token\n",
        "        if word == '' or word == 'endseq':\n",
        "            break\n",
        "\n",
        "        # Append the word to the current text\n",
        "        in_text += ' ' + word\n",
        "\n",
        "    # Remove the start token and return the caption\n",
        "    final = in_text.replace('startseq', '')\n",
        "\n",
        "    return final.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mJB8n4RLc7m4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, descriptions, features, tokenizer, max_length):\n",
        "    actual, predicted = [], []\n",
        "\n",
        "    # Generate captions for all test images\n",
        "    for image_id, desc_list in descriptions.items():\n",
        "        if image_id not in features:\n",
        "            continue\n",
        "\n",
        "        # Generate description\n",
        "        yhat = generate_desc(model, tokenizer, features[image_id], max_length)\n",
        "\n",
        "        # Store actual and predicted\n",
        "        references = [d.split() for d in desc_list]\n",
        "        # Clean references by removing start and end tokens\n",
        "        clean_references = []\n",
        "        for ref in references:\n",
        "            ref = [word for word in ref if word not in ('startseq', 'endseq')]\n",
        "            clean_references.append(ref)\n",
        "\n",
        "        # Clean hypothesis\n",
        "        hypothesis = yhat.split()\n",
        "\n",
        "        # Skip empty hypotheses\n",
        "        if len(hypothesis) == 0:\n",
        "            continue\n",
        "\n",
        "        # Add to list\n",
        "        actual.append(clean_references)\n",
        "        predicted.append(hypothesis)\n",
        "\n",
        "    # Apply smoothing to avoid division by zero\n",
        "    from nltk.translate.bleu_score import SmoothingFunction\n",
        "    smooth = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU scores with smoothing and error handling\n",
        "    try:\n",
        "        bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),\n",
        "                           smoothing_function=smooth)\n",
        "        bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),\n",
        "                           smoothing_function=smooth)\n",
        "        bleu3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),\n",
        "                           smoothing_function=smooth)\n",
        "        bleu4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                           smoothing_function=smooth)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BLEU scores: {e}\")\n",
        "        # Return zeros on error\n",
        "        return 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "    return bleu1, bleu2, bleu3, bleu4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uBnbUKWFdDSu"
      },
      "outputs": [],
      "source": [
        "class RLCaptioningAgent:\n",
        "    def __init__(self, model, tokenizer, max_length, vocab_size):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    def get_reward(self, generated_desc, reference_desc):\n",
        "        \"\"\"\n",
        "        Calculate reward based on BLEU score with error handling\n",
        "        \"\"\"\n",
        "        from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "        # Clean up descriptions by removing tokens\n",
        "        generated_words = [w for w in generated_desc.split() if w not in ('startseq', 'endseq')]\n",
        "        reference_words = [[w for w in desc.split() if w not in ('startseq', 'endseq')]\n",
        "                          for desc in reference_desc]\n",
        "        print(f\"Generated words: {generated_words}\")\n",
        "        print(f\"Reference words: {reference_words}\")\n",
        "\n",
        "        # Check for empty sequences which could cause division by zero\n",
        "        if len(generated_words) == 0:\n",
        "            return 0.0  # No words generated, return zero reward\n",
        "\n",
        "        # Calculate BLEU score with appropriate weights and smoothing\n",
        "        try:\n",
        "            from nltk.translate.bleu_score import SmoothingFunction\n",
        "            smooth = SmoothingFunction().method1  # Apply smoothing to avoid 0/0 fractions\n",
        "            bleu_score = sentence_bleu(reference_words, generated_words,\n",
        "                                    weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                                    smoothing_function=smooth)\n",
        "            return bleu_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating BLEU score: {e}\")\n",
        "            return 0.0  # Return zero reward on error\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, feature, input_seq, target_seq):\n",
        "        \"\"\"\n",
        "        Alternative training step implementation using custom loss calculation\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            predictions = self.model([feature, input_seq])  # Shape (1, seq_len, vocab_size)\n",
        "\n",
        "            # Reshape target to have batch dimension if it doesn't\n",
        "            if len(target_seq.shape) == 1:\n",
        "                target_seq = tf.expand_dims(target_seq, 0)  # Add batch dimension\n",
        "\n",
        "            # One-hot encode target sequences\n",
        "            target_one_hot = tf.one_hot(target_seq, depth=self.vocab_size)\n",
        "\n",
        "            # Compute cross-entropy loss manually\n",
        "            log_probs = tf.math.log(tf.clip_by_value(predictions, 1e-10, 1.0))\n",
        "            loss = -tf.reduce_sum(target_one_hot * log_probs, axis=-1)\n",
        "\n",
        "            # Create mask to ignore padding (0)\n",
        "            mask = tf.cast(tf.not_equal(target_seq, 0), dtype=tf.float32)\n",
        "\n",
        "            # Apply mask and calculate mean loss\n",
        "            masked_loss = loss * mask\n",
        "            loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        # Apply gradients\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self, features, descriptions, epochs=10, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the reinforcement learning model with proper tensor handling\n",
        "        \"\"\"\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            total_loss = 0.0\n",
        "            batch_count = 0\n",
        "\n",
        "            # Shuffle data\n",
        "            image_ids = list(features.keys())\n",
        "            random.shuffle(image_ids)\n",
        "\n",
        "            # Process in batches\n",
        "            for i in range(0, len(image_ids), batch_size):\n",
        "                batch_ids = image_ids[i:min(i+batch_size, len(image_ids))]\n",
        "                batch_loss = 0.0\n",
        "                valid_samples = 0\n",
        "\n",
        "                # Process each image in batch\n",
        "                for image_id in batch_ids:\n",
        "                    if image_id not in descriptions:\n",
        "                        continue\n",
        "\n",
        "                    feature = features[image_id]\n",
        "                    if isinstance(feature, np.ndarray):\n",
        "                        # Convert to tensor and ensure shape (1, 4096)\n",
        "                        if len(feature.shape) == 1:  # (4096,)\n",
        "                            feature = tf.convert_to_tensor(feature.reshape(1, -1))\n",
        "                        elif feature.shape[0] > 1 and len(feature.shape) > 1:  # (4096, 1)\n",
        "                            feature = tf.convert_to_tensor(np.expand_dims(feature, 0))  # Reshape to (1, 4096)\n",
        "                        else:\n",
        "                            feature = tf.convert_to_tensor(feature)  # Already (1, 4096)\n",
        "\n",
        "                    # Process each description for this image\n",
        "                    for desc in descriptions[image_id]:\n",
        "                        # Tokenize the description\n",
        "                        seq = self.tokenizer.texts_to_sequences([desc])[0]\n",
        "\n",
        "                        # Skip if sequence is too short\n",
        "                        if len(seq) < 2:\n",
        "                            continue\n",
        "\n",
        "                        # Create input and target sequences\n",
        "                        input_seq = seq[:-1]  # all words except last\n",
        "                        target_seq = seq[1:]  # all words except first\n",
        "\n",
        "                        # Pad input sequence\n",
        "                        input_seq = pad_sequences([input_seq], maxlen=self.max_length)[0]\n",
        "\n",
        "                        # Convert to tensors\n",
        "                        input_seq = tf.convert_to_tensor([input_seq])\n",
        "                        target_seq = tf.convert_to_tensor(target_seq)\n",
        "\n",
        "                        try:\n",
        "                            # Perform training step\n",
        "                            loss = self.train_step(feature, input_seq, target_seq)\n",
        "                            batch_loss += loss\n",
        "                            valid_samples += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error during training: {e}\")\n",
        "                            print(f\"Feature shape: {feature.shape if hasattr(feature, 'shape') else 'Unknown'}\")\n",
        "                            print(f\"Input sequence shape: {input_seq.shape}\")\n",
        "                            print(f\"Target sequence shape: {target_seq.shape}\")\n",
        "                            continue\n",
        "\n",
        "                # Calculate average batch loss\n",
        "                if valid_samples > 0:\n",
        "                    avg_batch_loss = batch_loss / valid_samples\n",
        "                    total_loss += avg_batch_loss\n",
        "                    batch_count += 1\n",
        "                    print(f\"  Batch {batch_count}, Loss: {avg_batch_loss:.4f}\")\n",
        "\n",
        "            # Calculate average epoch loss\n",
        "            if batch_count > 0:\n",
        "                epoch_loss = total_loss / batch_count\n",
        "                history.append(epoch_loss)\n",
        "                print(f\"  Epoch {epoch+1}/{epochs}, Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h_y4V45VdHA9",
        "outputId": "81f4ee16-84f0-43e1-f748-1f476c216885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Flickr8k dataset...\n",
            "Downloading image dataset...\n",
            "Image dataset downloaded and extracted.\n",
            "Downloading text dataset...\n",
            "Text dataset downloaded and extracted.\n",
            "Loaded descriptions: 8092\n",
            "Training set size: 6000\n",
            "Using 1000 images for training\n",
            "Descriptions loaded: 1000\n",
            "VGG16 model loaded for feature extraction\n",
            "Using a sample of 1000 images from 8091 total images\n",
            "Processing image 0/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_410']\n",
            "Received: inputs=Tensor(shape=(1, 224, 224, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image 100/1000\n",
            "Processing image 200/1000\n",
            "Processing image 300/1000\n",
            "Processing image 400/1000\n",
            "Processing image 500/1000\n",
            "Processing image 600/1000\n",
            "Processing image 700/1000\n",
            "Processing image 800/1000\n",
            "Processing image 900/1000\n",
            "Features extracted for 1000 images\n",
            "Features extracted: 1000\n",
            "Overlap between descriptions and features: 125 images\n",
            "Vocabulary size: 3139\n",
            "Maximum sequence length: 29\n",
            "Training data shapes: (6795, 4096) (6795, 29) (6795, 3139)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_32            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_31            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m803,584\u001b[0m │ input_layer_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_layer_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_8 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │      \u001b[38;5;34m1,048,832\u001b[0m │ dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m525,312\u001b[0m │ dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                           │                        │                │ not_equal_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ lstm_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m65,792\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3139\u001b[0m)           │        \u001b[38;5;34m806,723\u001b[0m │ dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_32            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_31            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">803,584</span> │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │ dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                           │                        │                │ not_equal_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ lstm_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3139</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">806,723</span> │ dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,250,243\u001b[0m (12.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,250,243</span> (12.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,250,243\u001b[0m (12.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,250,243</span> (12.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Training base model...\n",
            "Epoch 1/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 6.1133\n",
            "Epoch 2/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.4986\n",
            "Epoch 3/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.6658\n",
            "Epoch 4/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 3.1919\n",
            "Epoch 5/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2.8014\n",
            "Epoch 6/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.5396\n",
            "Epoch 7/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.3067\n",
            "Epoch 8/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.1100\n",
            "Epoch 9/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2.0207\n",
            "Epoch 10/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.8868\n",
            "Epoch 11/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.8039\n",
            "Epoch 12/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.6900\n",
            "Epoch 13/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.6297\n",
            "Epoch 14/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.5317\n",
            "Epoch 15/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.4693\n",
            "Epoch 16/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.3771\n",
            "Epoch 17/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.3164\n",
            "Epoch 18/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.2224\n",
            "Epoch 19/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.1829\n",
            "Epoch 20/20\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.1113\n",
            "Test set size: 1000\n",
            "Using 100 images for testing\n",
            "VGG16 model loaded for feature extraction\n",
            "Using a sample of 100 images from 8091 total images\n",
            "Processing image 0/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_444']\n",
            "Received: inputs=Tensor(shape=(1, 224, 224, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted for 100 images\n",
            "Evaluating base model...\n",
            "Base Model BLEU Scores: 0.7143, 0.5976, 0.2271, 0.1156\n",
            "Fine-tuning with reinforcement learning...\n",
            "Epoch 1/5\n",
            "  Batch 1, Loss: 11.4439\n",
            "  Batch 2, Loss: 10.5999\n",
            "  Batch 3, Loss: 10.1741\n",
            "  Batch 4, Loss: 9.2907\n",
            "  Batch 5, Loss: 8.4655\n",
            "  Batch 6, Loss: 9.1889\n",
            "  Batch 7, Loss: 7.0164\n",
            "  Batch 8, Loss: 7.5543\n",
            "  Batch 9, Loss: 7.1597\n",
            "  Batch 10, Loss: 8.7772\n",
            "  Batch 11, Loss: 5.7017\n",
            "  Batch 12, Loss: 6.8818\n",
            "  Batch 13, Loss: 6.2479\n",
            "  Batch 14, Loss: 5.6428\n",
            "  Batch 15, Loss: 5.1285\n",
            "  Batch 16, Loss: 5.5956\n",
            "  Batch 17, Loss: 4.8653\n",
            "  Batch 18, Loss: 4.8789\n",
            "  Batch 19, Loss: 5.4718\n",
            "  Batch 20, Loss: 4.8471\n",
            "  Batch 21, Loss: 5.4600\n",
            "  Batch 22, Loss: 5.4996\n",
            "  Batch 23, Loss: 4.9299\n",
            "  Batch 24, Loss: 4.9481\n",
            "  Batch 25, Loss: 5.0075\n",
            "  Batch 26, Loss: 5.1135\n",
            "  Batch 27, Loss: 5.3929\n",
            "  Batch 28, Loss: 4.6762\n",
            "  Batch 29, Loss: 5.4726\n",
            "  Batch 30, Loss: 5.9683\n",
            "  Batch 31, Loss: 5.1100\n",
            "  Batch 32, Loss: 5.5960\n",
            "  Batch 33, Loss: 4.6569\n",
            "  Batch 34, Loss: 5.0082\n",
            "  Batch 35, Loss: 4.9053\n",
            "  Batch 36, Loss: 4.8760\n",
            "  Batch 37, Loss: 4.7690\n",
            "  Batch 38, Loss: 5.0037\n",
            "  Batch 39, Loss: 5.0458\n",
            "  Batch 40, Loss: 4.7766\n",
            "  Batch 41, Loss: 5.2608\n",
            "  Batch 42, Loss: 4.3261\n",
            "  Batch 43, Loss: 4.4141\n",
            "  Batch 44, Loss: 4.3452\n",
            "  Batch 45, Loss: 4.6056\n",
            "  Batch 46, Loss: 4.5907\n",
            "  Batch 47, Loss: 4.4486\n",
            "  Batch 48, Loss: 4.2834\n",
            "  Batch 49, Loss: 4.5488\n",
            "  Batch 50, Loss: 5.2476\n",
            "  Batch 51, Loss: 4.6119\n",
            "  Batch 52, Loss: 4.1088\n",
            "  Batch 53, Loss: 4.2793\n",
            "  Batch 54, Loss: 4.0936\n",
            "  Batch 55, Loss: 4.5140\n",
            "  Epoch 1/5, Average Loss: 5.7241\n",
            "Epoch 2/5\n",
            "  Batch 1, Loss: 4.1698\n",
            "  Batch 2, Loss: 4.3087\n",
            "  Batch 3, Loss: 3.8943\n",
            "  Batch 4, Loss: 4.3809\n",
            "  Batch 5, Loss: 3.8735\n",
            "  Batch 6, Loss: 3.7003\n",
            "  Batch 7, Loss: 4.4858\n",
            "  Batch 8, Loss: 4.0246\n",
            "  Batch 9, Loss: 3.5085\n",
            "  Batch 10, Loss: 4.4366\n",
            "  Batch 11, Loss: 4.0999\n",
            "  Batch 12, Loss: 4.3119\n",
            "  Batch 13, Loss: 4.4587\n",
            "  Batch 14, Loss: 3.8884\n",
            "  Batch 15, Loss: 3.8211\n",
            "  Batch 16, Loss: 3.9429\n",
            "  Batch 17, Loss: 3.9007\n",
            "  Batch 18, Loss: 3.9820\n",
            "  Batch 19, Loss: 3.3500\n",
            "  Batch 20, Loss: 4.0507\n",
            "  Batch 21, Loss: 3.0069\n",
            "  Batch 22, Loss: 3.9124\n",
            "  Batch 23, Loss: 3.6758\n",
            "  Batch 24, Loss: 3.9460\n",
            "  Batch 25, Loss: 3.8773\n",
            "  Batch 26, Loss: 3.9864\n",
            "  Batch 27, Loss: 3.5124\n",
            "  Batch 28, Loss: 2.9240\n",
            "  Batch 29, Loss: 4.0089\n",
            "  Batch 30, Loss: 3.9811\n",
            "  Batch 31, Loss: 3.6899\n",
            "  Batch 32, Loss: 3.9369\n",
            "  Batch 33, Loss: 3.9547\n",
            "  Batch 34, Loss: 3.8227\n",
            "  Batch 35, Loss: 3.7982\n",
            "  Batch 36, Loss: 4.2190\n",
            "  Batch 37, Loss: 3.8133\n",
            "  Batch 38, Loss: 4.0783\n",
            "  Batch 39, Loss: 4.2493\n",
            "  Batch 40, Loss: 3.6445\n",
            "  Batch 41, Loss: 3.7719\n",
            "  Batch 42, Loss: 4.0707\n",
            "  Batch 43, Loss: 3.4809\n",
            "  Batch 44, Loss: 4.0965\n",
            "  Batch 45, Loss: 3.9180\n",
            "  Batch 46, Loss: 3.8604\n",
            "  Batch 47, Loss: 3.7203\n",
            "  Batch 48, Loss: 3.9110\n",
            "  Batch 49, Loss: 4.0612\n",
            "  Batch 50, Loss: 3.6884\n",
            "  Batch 51, Loss: 3.6537\n",
            "  Batch 52, Loss: 3.8764\n",
            "  Epoch 2/5, Average Loss: 3.8988\n",
            "Epoch 3/5\n",
            "  Batch 1, Loss: 3.7927\n",
            "  Batch 2, Loss: 3.7516\n",
            "  Batch 3, Loss: 3.9134\n",
            "  Batch 4, Loss: 3.3465\n",
            "  Batch 5, Loss: 3.5752\n",
            "  Batch 6, Loss: 3.6282\n",
            "  Batch 7, Loss: 3.4464\n",
            "  Batch 8, Loss: 3.5664\n",
            "  Batch 9, Loss: 3.7528\n",
            "  Batch 10, Loss: 3.6062\n",
            "  Batch 11, Loss: 3.5151\n",
            "  Batch 12, Loss: 3.5855\n",
            "  Batch 13, Loss: 3.5234\n",
            "  Batch 14, Loss: 3.5577\n",
            "  Batch 15, Loss: 3.3232\n",
            "  Batch 16, Loss: 3.6180\n",
            "  Batch 17, Loss: 3.3824\n",
            "  Batch 18, Loss: 3.5264\n",
            "  Batch 19, Loss: 3.0857\n",
            "  Batch 20, Loss: 3.6526\n",
            "  Batch 21, Loss: 3.7522\n",
            "  Batch 22, Loss: 3.2346\n",
            "  Batch 23, Loss: 3.4810\n",
            "  Batch 24, Loss: 3.5280\n",
            "  Batch 25, Loss: 3.5713\n",
            "  Batch 26, Loss: 3.4645\n",
            "  Batch 27, Loss: 3.5644\n",
            "  Batch 28, Loss: 3.1440\n",
            "  Batch 29, Loss: 3.3865\n",
            "  Batch 30, Loss: 3.2686\n",
            "  Batch 31, Loss: 3.4472\n",
            "  Batch 32, Loss: 3.5575\n",
            "  Batch 33, Loss: 3.6537\n",
            "  Batch 34, Loss: 3.7943\n",
            "  Batch 35, Loss: 2.9559\n",
            "  Batch 36, Loss: 3.3749\n",
            "  Batch 37, Loss: 3.3552\n",
            "  Batch 38, Loss: 3.3726\n",
            "  Batch 39, Loss: 3.7698\n",
            "  Batch 40, Loss: 3.5240\n",
            "  Batch 41, Loss: 3.3589\n",
            "  Batch 42, Loss: 3.8097\n",
            "  Batch 43, Loss: 3.7154\n",
            "  Batch 44, Loss: 3.5346\n",
            "  Batch 45, Loss: 3.5445\n",
            "  Batch 46, Loss: 3.1723\n",
            "  Batch 47, Loss: 3.5111\n",
            "  Batch 48, Loss: 3.4854\n",
            "  Batch 49, Loss: 3.4192\n",
            "  Batch 50, Loss: 2.8140\n",
            "  Batch 51, Loss: 3.5504\n",
            "  Batch 52, Loss: 3.7030\n",
            "  Batch 53, Loss: 3.6551\n",
            "  Batch 54, Loss: 3.7141\n",
            "  Epoch 3/5, Average Loss: 3.5062\n",
            "Epoch 4/5\n",
            "  Batch 1, Loss: 3.1492\n",
            "  Batch 2, Loss: 3.3338\n",
            "  Batch 3, Loss: 3.4321\n",
            "  Batch 4, Loss: 3.7342\n",
            "  Batch 5, Loss: 3.1849\n",
            "  Batch 6, Loss: 3.3551\n",
            "  Batch 7, Loss: 3.1269\n",
            "  Batch 8, Loss: 3.3281\n",
            "  Batch 9, Loss: 3.4027\n",
            "  Batch 10, Loss: 2.9906\n",
            "  Batch 11, Loss: 3.4019\n",
            "  Batch 12, Loss: 3.3235\n",
            "  Batch 13, Loss: 3.3309\n",
            "  Batch 14, Loss: 3.6220\n",
            "  Batch 15, Loss: 3.3180\n",
            "  Batch 16, Loss: 3.2881\n",
            "  Batch 17, Loss: 3.3099\n",
            "  Batch 18, Loss: 3.3551\n",
            "  Batch 19, Loss: 3.1528\n",
            "  Batch 20, Loss: 3.5806\n",
            "  Batch 21, Loss: 3.4141\n",
            "  Batch 22, Loss: 3.4289\n",
            "  Batch 23, Loss: 3.5036\n",
            "  Batch 24, Loss: 3.1597\n",
            "  Batch 25, Loss: 3.3511\n",
            "  Batch 26, Loss: 3.5419\n",
            "  Batch 27, Loss: 3.1159\n",
            "  Batch 28, Loss: 3.5441\n",
            "  Batch 29, Loss: 3.4683\n",
            "  Batch 30, Loss: 3.4662\n",
            "  Batch 31, Loss: 3.6575\n",
            "  Batch 32, Loss: 2.9679\n",
            "  Batch 33, Loss: 3.5138\n",
            "  Batch 34, Loss: 3.5415\n",
            "  Batch 35, Loss: 3.2079\n",
            "  Batch 36, Loss: 3.1988\n",
            "  Batch 37, Loss: 3.4182\n",
            "  Batch 38, Loss: 3.4116\n",
            "  Batch 39, Loss: 3.2716\n",
            "  Batch 40, Loss: 2.8849\n",
            "  Batch 41, Loss: 3.2893\n",
            "  Batch 42, Loss: 3.3391\n",
            "  Batch 43, Loss: 3.2539\n",
            "  Batch 44, Loss: 3.7107\n",
            "  Batch 45, Loss: 3.2274\n",
            "  Batch 46, Loss: 3.6161\n",
            "  Batch 47, Loss: 3.3501\n",
            "  Batch 48, Loss: 3.0329\n",
            "  Batch 49, Loss: 3.5368\n",
            "  Batch 50, Loss: 3.3064\n",
            "  Batch 51, Loss: 3.3470\n",
            "  Batch 52, Loss: 2.9042\n",
            "  Batch 53, Loss: 3.5393\n",
            "  Epoch 4/5, Average Loss: 3.3442\n",
            "Epoch 5/5\n",
            "  Batch 1, Loss: 3.4564\n",
            "  Batch 2, Loss: 3.4695\n",
            "  Batch 3, Loss: 3.0385\n",
            "  Batch 4, Loss: 2.9773\n",
            "  Batch 5, Loss: 3.5016\n",
            "  Batch 6, Loss: 3.3840\n",
            "  Batch 7, Loss: 2.8920\n",
            "  Batch 8, Loss: 2.9750\n",
            "  Batch 9, Loss: 3.3102\n",
            "  Batch 10, Loss: 3.4502\n",
            "  Batch 11, Loss: 3.4486\n",
            "  Batch 12, Loss: 3.2621\n",
            "  Batch 13, Loss: 3.4832\n",
            "  Batch 14, Loss: 3.0990\n",
            "  Batch 15, Loss: 3.2073\n",
            "  Batch 16, Loss: 3.5184\n",
            "  Batch 17, Loss: 3.3822\n",
            "  Batch 18, Loss: 3.3213\n",
            "  Batch 19, Loss: 3.3462\n",
            "  Batch 20, Loss: 3.1219\n",
            "  Batch 21, Loss: 3.2745\n",
            "  Batch 22, Loss: 3.2576\n",
            "  Batch 23, Loss: 3.3532\n",
            "  Batch 24, Loss: 3.2650\n",
            "  Batch 25, Loss: 3.1074\n",
            "  Batch 26, Loss: 3.2238\n",
            "  Batch 27, Loss: 3.2957\n",
            "  Batch 28, Loss: 3.3474\n",
            "  Batch 29, Loss: 3.0704\n",
            "  Batch 30, Loss: 3.1409\n",
            "  Batch 31, Loss: 2.9120\n",
            "  Batch 32, Loss: 3.2473\n",
            "  Batch 33, Loss: 3.3068\n",
            "  Batch 34, Loss: 3.2895\n",
            "  Batch 35, Loss: 3.3447\n",
            "  Batch 36, Loss: 3.4774\n",
            "  Batch 37, Loss: 3.1035\n",
            "  Batch 38, Loss: 3.4776\n",
            "  Batch 39, Loss: 3.5410\n",
            "  Batch 40, Loss: 3.3247\n",
            "  Batch 41, Loss: 3.0640\n",
            "  Batch 42, Loss: 3.0574\n",
            "  Batch 43, Loss: 3.3110\n",
            "  Batch 44, Loss: 3.3835\n",
            "  Batch 45, Loss: 3.3294\n",
            "  Batch 46, Loss: 3.1135\n",
            "  Batch 47, Loss: 3.2037\n",
            "  Batch 48, Loss: 3.4144\n",
            "  Batch 49, Loss: 3.2902\n",
            "  Batch 50, Loss: 2.9585\n",
            "  Batch 51, Loss: 3.3477\n",
            "  Batch 52, Loss: 3.2065\n",
            "  Batch 53, Loss: 3.1130\n",
            "  Batch 54, Loss: 3.5676\n",
            "  Epoch 5/5, Average Loss: 3.2660\n",
            "Evaluating RL model...\n",
            "RL Model BLEU Scores: 0.5000, 0.4082, 0.2647, 0.0978\n",
            "\n",
            "Sample captions:\n",
            "Image 1 (3123463486_f5b36a3624):\n",
            "  References:\n",
            "    - a brown black and white dog runs along on the gravel\n",
            "    - a dog runs\n",
            "    - a little dog running on sand\n",
            "    - the brown white and black dog runs on gravel surface\n",
            "    - the dog is running across the gravel\n",
            "  Base model: a dog and white dog is playing on the snow and white dog in the snow\n",
            "  RL model: a dog and white dog is playing on the snow and white dog in the snow\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Create a directory to save models\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "\n",
        "    # Download and prepare dataset\n",
        "    download_and_extract_dataset()\n",
        "\n",
        "    # Load and clean descriptions\n",
        "    filename = '/content/data/Flickr8k.token.txt'\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Error: {filename} not found!\")\n",
        "        return\n",
        "\n",
        "    descriptions = load_descriptions(filename)\n",
        "    print('Loaded descriptions:', len(descriptions))\n",
        "\n",
        "    # Clean descriptions\n",
        "    descriptions = clean_descriptions(descriptions)\n",
        "\n",
        "    # Save descriptions\n",
        "    save_descriptions(descriptions, 'descriptions.txt')\n",
        "\n",
        "    # Load training set (use more images for better results)\n",
        "    filename = '/content/data/Flickr_8k.trainImages.txt'\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Error: {filename} not found!\")\n",
        "        return\n",
        "\n",
        "    train = load_set(filename)\n",
        "    print('Training set size:', len(train))\n",
        "\n",
        "    # Use more images for training (500 instead of 100)\n",
        "    train_sample_size = 1000\n",
        "    train_sample = list(train)[:train_sample_size]\n",
        "    print(f'Using {len(train_sample)} images for training')\n",
        "\n",
        "    # Load clean descriptions\n",
        "    train_descriptions = load_clean_descriptions('descriptions.txt', train_sample)\n",
        "    print('Descriptions loaded:', len(train_descriptions))\n",
        "\n",
        "    # Extract features\n",
        "    image_dir = '/content/data/Flicker8k_Dataset'\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Error: Image directory {image_dir} not found!\")\n",
        "        return\n",
        "\n",
        "    # Extract features for training images\n",
        "    train_features = extract_features(image_dir, sample_size=train_sample_size)\n",
        "    print('Features extracted:', len(train_features))\n",
        "\n",
        "    # Check overlap between descriptions and features\n",
        "    overlap = set(train_descriptions.keys()) & set(train_features.keys())\n",
        "    print(f'Overlap between descriptions and features: {len(overlap)} images')\n",
        "\n",
        "    # Prepare tokenizer\n",
        "    all_desc = []\n",
        "    for key in train_descriptions.keys():\n",
        "        [all_desc.append(d) for d in train_descriptions[key]]\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(all_desc)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('Vocabulary size:', vocab_size)\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    max_length = max(len(d.split()) for d in all_desc)\n",
        "    print('Maximum sequence length:', max_length)\n",
        "\n",
        "    # Save tokenizer\n",
        "    with open('tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    # Prepare data for training\n",
        "    X1, X2, y = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
        "    print('Training data shapes:', X1.shape, X2.shape, y.shape)\n",
        "\n",
        "    # Define the model\n",
        "    model = define_model(vocab_size, max_length)\n",
        "\n",
        "    # Train the base model\n",
        "    print(\"Training base model...\")\n",
        "    model.fit([X1, X2], y, epochs=20, batch_size=64, verbose=1)\n",
        "\n",
        "    # Save the base model\n",
        "    model.save('models/model_base.keras')\n",
        "\n",
        "    # Load test set for evaluation\n",
        "    filename = '/content/data/Flickr_8k.testImages.txt'\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Error: {filename} not found!\")\n",
        "        return\n",
        "\n",
        "    test = load_set(filename)\n",
        "    print('Test set size:', len(test))\n",
        "\n",
        "    # Use a subset of test images for evaluation\n",
        "    test_sample_size = 100\n",
        "    test_sample = list(test)[:test_sample_size]\n",
        "    print(f'Using {len(test_sample)} images for testing')\n",
        "\n",
        "    # Load descriptions for test images\n",
        "    test_descriptions = load_clean_descriptions('descriptions.txt', test_sample)\n",
        "\n",
        "    # Extract features for test images\n",
        "    test_features = extract_features(image_dir, sample_size=test_sample_size)\n",
        "\n",
        "    # Evaluate base model\n",
        "    print(\"Evaluating base model...\")\n",
        "    bleu1, bleu2, bleu3, bleu4 = evaluate_model(\n",
        "        model, test_descriptions, test_features, tokenizer, max_length)\n",
        "    print(f'Base Model BLEU Scores: {bleu1:.4f}, {bleu2:.4f}, {bleu3:.4f}, {bleu4:.4f}')\n",
        "\n",
        "    # Fine-tune with RL\n",
        "    print(\"Fine-tuning with reinforcement learning...\")\n",
        "    rl_agent = RLCaptioningAgent(model, tokenizer, max_length, vocab_size)\n",
        "    history = rl_agent.train(train_features, train_descriptions, epochs=5, batch_size=16)\n",
        "\n",
        "    # Save the RL fine-tuned model\n",
        "    rl_agent.model.save('models/model_rl.keras')\n",
        "\n",
        "    # Evaluate RL model\n",
        "    print(\"Evaluating RL model...\")\n",
        "    bleu1, bleu2, bleu3, bleu4 = evaluate_model(\n",
        "        rl_agent.model, test_descriptions, test_features, tokenizer, max_length)\n",
        "    print(f'RL Model BLEU Scores: {bleu1:.4f}, {bleu2:.4f}, {bleu3:.4f}, {bleu4:.4f}')\n",
        "\n",
        "    print(\"\\nSample captions:\")\n",
        "    count = 0\n",
        "\n",
        "    for image_id in test_features.keys():\n",
        "        if count == 5:\n",
        "            break\n",
        "\n",
        "        # Get reference captions\n",
        "        refs = [' '.join(ref.split()[1:-1]) for ref in test_descriptions.get(image_id, [])]\n",
        "\n",
        "        if refs:  # Proceed only if reference captions exist\n",
        "            # Generate caption using base model\n",
        "            base_caption = generate_desc(model, tokenizer, test_features[image_id], max_length)\n",
        "            # Generate caption using RL model\n",
        "            rl_caption = generate_desc(rl_agent.model, tokenizer, test_features[image_id], max_length)\n",
        "\n",
        "            print(f\"Image {count+1} ({image_id}):\")\n",
        "            print(\"  References:\")\n",
        "            for ref in refs:\n",
        "                print(f\"    - {ref}\")  # Print all reference captions\n",
        "\n",
        "            print(f\"  Base model: {base_caption}\")\n",
        "            print(f\"  RL model: {rl_caption}\")\n",
        "            print()\n",
        "\n",
        "            count += 1  # Increment counter\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pYqOaGXoOsSj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}